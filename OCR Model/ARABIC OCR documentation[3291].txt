#Tesseract-OCR for ACO materials (@AUC), by Mark Muehlhaeusler

##Installation

There is little to no linux support available in our offices; luckily, the Librarians at Mannheim have created 
a Tesseract-OCR package for windows, which can be dowloaded [from GitHub](https://github.com/UB-Mannheim/tesseract/wiki).

It comes with an installer, so the actual installation process is 
straightforward. Note, however, that additional language packs have to be selected during the installation
process. 

After installation, you may need to set permissions to read and write to the Tesseract-OCR directory and 
its folders for your account (or get an admin to do so for you). This depends on where you want to keep the 
output files. See [this page](http://www.wikihow.com/Change-File-Permissions-on-Windows-7) for instructions.

Also, he downloading the Arabic package failed during our installation. While Tesseract was able to 
perform OCR in English and Hebrew, this was not the case for Arabic. 

The traineddata files had to be downloaded manually, and moved to the TESSDATA folder. It appears that 
there are various traineddata packages for different versions of Tesseract. You can check which version you 
have installed by tying 

```
tesseract -v
```

..at the commant prompt (you may need to be in the Tesseract-OCR directory to do this. 

For version 3.04 and 3.05, there is a set of eight files for Arabic, which have to be copied to the 
TESSDATA folder. These, and other files are available [here] (https://github.com/tesseract-ocr/tessdata/tree/3.04.00)

They include: 
-ara.cube.bigrams
-ara.cube.fold
-ara.cube.lm
-ara.cube.nn
-ara.cube.params
-ara.cube.size
-ara.cube.word-freq
-ara.traineddata

There is no 'download' button for some of these files at the site above. I downloaded the raw files
(by clicking on 'raw', then right-clicking on the screen, and selecting "save as...".). This meant 
that I had to change the file extensions by opening the files in Notepad++, and "saving copies as...":
Notepad++ offers the option of saving as *.properties file; the last part of ara.cube.nn therefore 
becomes the file extension. If you look at the files in Windows explorer, you should see 
"BIGRAMS file" as the "file type" for the first file.

##Testing single image conversions from the Windows command line

For single image conversions of Arabic scanned pages, you can use this syntax at the Windows command line:

```
tesseract "C:\Program Files\Tesseract-OCR\imgs\scan1.jpg" "C:\Program Files\Tesseract-OCR\outs\out1.txt" -l ara
```

..assuming that you have already created a folder "imgs" at the path above, with file "scan1.jpg" inside, and 
assuming also that you have created the "outs" folder at the path specified. 

With some luck, you should see the .txt file appear in the output folder. Strictly speaking, it is not necessary to specify the format of the output file. The default generated by Tesseract is ".txt".

##Other output

Tesseract can also output searchable PDFs, using this kind of syntax:

tesseract source.png out -l ara PDF

Reading FROM PDF does not appear to be supported. 

Related, hOCR output is available: This structured output provides coordinates of words in an image paired with the recognized text, thus:

<span class='ocrx_word' id='word_1_38' title='bbox 669 316 674 317; x_wconf 82'>[[OCR TEXT HERE]]</span>



##Batch processing Arabic image files for OCR\imgs\*

I followed the instructions at the URL below for a simple batch process. 

[https://digitalaladore.wordpress.com/2014/11/17/using-tesseract-via-command-line/](https://digitalaladore.wordpress.com/2014/11/17/using-tesseract-via-command-line/)

Since r/w permissions are an issue on the different computers where this was tested, I decided to keep 
images and outputs in dedicated folders within in the Tesseract-OCR main folder (entitled, "imgs" and "outs",
creatively). 

The recipe calls for a batch script to be created, with the following content:

```
@Echo off
Set _SourcePath="C:\Program Files\Tesseract-OCR\imgs\*.jpg"
Set _OutputPath="C:\Program Files\Tesseract-OCR\outs\ "
Set _Tesseract="C:\Program Files\Tesseract-OCR\tesseract.exe"
For %%A in (%_SourcePath%) Do Echo Converting %%A...&%_Tesseract% "%%A" %_OutputPath%%%~nA -l ara
Set "_SourcePath="
Set "_OutputPath="
Set "_Tesseract="
```

The script as given at the URl above did not work for me initially. For some reason the quotation 
marks are needed in some places, but not in others -- hence the form given above.

Save this script as "batch_process.bat" to the Tesseract-OCR main folder. Double-clicking in windows explorer 
should run the script. Alternatively, in the windows command line, navigate to the Tesseract-OCR folder, and 
type "batch_process" at the command prompt. BINGO (hopefully).



##Image pre-processing

A command line tool like ImageMagick can be used to convert batches of images from TIFF to JPG, and from color to grayscale, if desired. 

Use this at the Windows command line to call ImageMagic:

```
>magick convert "C:\path.... input_file.jpg" -grayscale Rec709Luma "C:\path... output_file.jpg"
```

In tests with older Arabic material, conversion to greyscale resulted in less accurate OCR.


##HOCR file acquisition

Tesseract can generate HOCR output, that is, an XML file which includes recognized text AND the coordinates of each word in the original image. To retrieve this output, add -hocr to the end of the Tesseract command. 

The output format looks like this:

```
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
 <head>
  <title></title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name='ocr-system' content='tesseract 3.05.00dev' />
  <meta name='ocr-capabilities' content='ocr_page ocr_carea ocr_par ocr_line ocrx_word'/>
</head>
<body>
  <div class='ocr_page' id='page_1' title='image "C:\Program Files\Tesseract-OCR\imgs\scan1.jpg"; bbox 0 0 868 411; ppageno 0'>
   <div class='ocr_carea' id='block_1_1' title="bbox 1 3 855 402">
    <p class='ocr_par' dir='rtl' id='par_1_1' lang='ara' title="bbox 14 3 854 207">
     <span class='ocr_line' id='line_1_1' title="bbox 15 3 822 48; baseline -0.012 -12; x_size 31.5; x_descenders 6; x_ascenders 10.5"><span class='ocrx_word' id='word_1_1' title='bbox 712 3 822 39; x_wconf 34'>الطريقة..</span> <span class='ocrx_word' id='word_1_2' title='bbox 643 5 697 37; x_wconf 52'>يقبل</span> <span class='ocrx_word' id='word_1_3' title='bbox 570 5 630 41; x_wconf 83'>اللحم</span> <span class='ocrx_word' id='word_1_4' title='bbox 460 6 559 38; x_wconf 31'>باليصلة</span> <span class='ocrx_word' id='word_1_5' title='bbox 376 8 447 41; x_wconf 39'>المغرية</span> <span class='ocrx_word' id='word_1_6' title='bbox 288 10 366 46; x_wconf 56'>والثوم</span> <span class='ocrx_word' id='word_1_7' title='bbox 181 10 278 45; x_wconf 52'>والتوابل</span> <span class='ocrx_word' id='word_1_8' title='bbox 71 12 172 42; x_wconf 36'>وتتشكل</span> <span class='ocrx_word' id='word_1_9' title='bbox 15 13 61 48; x_wconf 100'>علي</span> 
     </span>
```

##Image extraction with hocr-tools

Use [hocr-extract-images] (https://github.com/tmbdev/hocr-tools#hocr-extract-images), which runs under Python 2.7. In order to install this package, you may need to download it from the GitHub site, unzip into a local folder, and then run the setup progam in a Windows command prompt

```
cd c:\[INSERT PATH HERE]\hocr-tools-master
python setup.py install
```

For some reason, I need to be in the folder of the program itself to run it, i.e. in the hocr-tools-master folder. A subfolder out\ contains the .hocr file generated by Tesseract-OCR, as well as the original .jpg. This folder in turn is given a subfolder "test" where the output is stored. The command format is as follows:

```
python hocr-extract-images -b outs -p outs\test\line-%03d.png -e ocr_word outs\scan1.hocr
```

...where the last bit points to the relative path of the .hocr file, which must have the same base name as the related image. 


he following snippet from the [test log run at Mannheim](https://travis-ci.org/UB-Mannheim/hocr-tools/jobs/168107164) shows the syntax in action.

```
# ocr_page argument
ok 1 - Executed: hocr-extract-images -p page-%03d.png -b ../testdata -e ocr_page ../testdata/tess.hocr
...
# ocr_line argument
ok 6 - Executed: hocr-extract-images -p line-%03d.png -b ../testdata -e ocr_line ../testdata/tess.hocr
...
# ocrx_word argument
ok 11 - Executed: hocr-extract-images -p word-%03d.png -b ../testdata -e ocrx_word ../testdata/tess.hocr
...
```



##NEXT: try some more pre-OCR image processing with ImageMagick

GARYSCALE || B/W CONVERSION

Here follows a list of files names, and the conversion methods by which they were derived from base.jpg (=auc_aco000136_000005_d). These files are held in the .\ImageMagick\Images\ACO folder.

gr_colorspace.jpg		> magick convert  in.jpg  -colorspace Gray   out.jpg
gr_709Luma.jpg			> magick convert  in.jpg  -grayscale rec709Luma out.jpg
gr_601Luma.jpg			> magick convert  in.jpg  -grayscale rec601Luma out.jpg
gr_modulate.jpg			> magick convert  in.jpg  -modulate 100,0 out.jpg #Takes a long time!
gr_intensity.jpg		> magick convert  in.jpg  -fx intensity out.jpg

bw_threshold75.jpg		> magick convert in.jpg	-threshold 75% out.jpg
bw_threshold70.jpg		> magick convert in.jpg	-threshold 70% out.jpg
bw_threshold60.jpg		> magick convert in.jpg	-threshold 60% out.jpg
bw_threshold55.jpg		> magick convert in.jpg	-threshold 55% out.jpg
bw_monochrome.jpg		> magick convert in.jpg  -monochrome out.jpg   #Takes a long time!

bw_thr75_sharp1.jpg		> magick convert in.jpg -threshold 75% - sharpen 0x1 out.jpg 
bw_thr75_sharp2.jpg		> magick convert in.jpg -threshold 75% - sharpen 0x1 out.jpg 
bw_thr75_sharp3.jpg		> magick convert in.jpg -threshold 75% - sharpen 0x1 out.jpg 

bw_thr75_ashar3_1.jpg	> magick convert in.jpg -threshold 75% -adaptive-sharpen 3x1 out.jpg
bw_thr75_ashar12_4.jpg	> magick convert in.jpg -threshold 75% -adaptive-sharpen 12x4 out.jpg
bw_ashar12x4_thr75.jpg	> magick convert in.jpg -adaptive-sharpen 12x4 -threshold 75%  out.jpg #long processing time!


bw_mono_ashar3_1.jpg	> magick convert in.jpg -monochrome -adaptive-sharpen 3x1 out.jpg
bw_mono_ashar12_4.jpg	> magick convert in.jpg -monochrome -adaptive-sharpen 12x4 out.jpg
bw_mono_ashar0_2.jpg	> magick convert in.jpg -monochrome -adaptive-sharpen 0x2 out.jpg	

bw_mono_sharp1.jpg		> magick convert in.jpg -monochrome -sharpen 0x1 out.jpg 
bw_mono_sharp2.jpg		> magick convert in.jpg -monochrome -sharpen 0x2 out.jpg 
bw_mono_sharp3.jpg		> magick convert in.jpg -monochrome -sharpen 0x3 out.jpg 

bw_dither.jpg			> magick convert in.jpg +dither -colors 2 colorspace gray -normalize out.jpg

bw_blur_thresh75.jpg	> magick convert in.jpg -blur 4x2 -threshold 75% out.jpg
bw_blur2_thresh75.jpg	> magick convert in.jpg -blur 2x1 -threshold 75% out.jpg
bw_blur3_thresh75.jpg	> magick convert in.jpg -blur 2x1 -sharpen 0x1 -threshold 75% out.jpg

bw_contr_thr75.jpg 		> magick convert in.jpg -contrast -contrast -threshold 75% out.jpg
bw_blur_contr_thr75		> magick convert in.jpg -blur 2x1 -contrast -threshold 75% out.jpg

bw_lat10.jpg			> magick convert in.jpg -colorspace gray -negate -lat 12x12+2% -threshold 75% out.jpg


In particular, go for "Adaptive Thresholding" (https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html)

I presume that this is the same as:

-adaptive-sharpen radius[xsigma]
Adaptively sharpen pixels, with increasing effect near edges.
A Gaussian operator of the given radius and standard deviation (sigma) is used. If sigma is not given it defaults to 1.

First forget the first number, *just use 0 which will then use the best number for the 'sigma' factor you give*. The larger the sigma the more it sharpens. SYNTAX:

  convert  A_blur.jpg         -adaptive-sharpen 0x1.0    A_blur_sharp.jpg

"http://www.imagemagick.org/discourse-server/viewtopic.php?f=1&t=10266" >>
"I played with -adaptive-sharpen and found that it does nothing if you specify radiusxsigma as 0xanyvalue (e.g. 0x4), it does little if one specifies radius alone (e.g. 4), but does work if one specifies radiusxsigma as radius=3*sigma and a value for sigma (e.g. 12x4). With the other radiusxsigma functions (blur and gaussian-blur), if one specifies radius=0, it supporsedly computes the radius from the sigma (typically 3*sigma for radius), but this does not seem to be working for adaptive-sharpen (or not well?). Use Anthony's flicker_cmp script to alternately display two or more images in rapid succession to compare the changes."


###################NEW NOT TESTED############

Try extracting images with Tesseract

https://www.quora.com/Is-it-possible-to-output-the-character-word-line-segmentation-using-Tesseract-OCR

"You can do this using the PageIterator* tesseract::TessBaseAPI::AnalyseLayout() API call—after setting up everything that is required, of course. The call returns a page iterator that lets you walk over all the page segments and do whatever you want with them.

You can also call the tesseract executable to process the image and output a textual representation of AnalyseLayout’s output. You can do this with

    tesseract infile.tiff outfile -l eng -psm 1 hocr

This command processes the input infile.tiff assuming English language (-l eng) and requesting automatic page segmentation (-psm 1); the output is produced in hOCR format, in a file named outfile.hocr.

The hOCR format is HTML-based, and therefore quite easy to process in any language with HTML readers (virtually every one). The representation is quite simple and well described in hOCR - OCR Workflow and Output embedded in HTML"

OR#########
https://stackoverflow.com/questions/28591117/how-do-i-segment-a-document-using-tesseract-then-output-the-resulting-bounding-b



